{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple MDP Design\n",
    "Grzegorz Malisz\n",
    "Student Number: 4852370\n",
    "[GitHub Repository](https://github.com/grzgm/deth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from math import isclose\n",
    "import copy\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP definition ver 1\n",
    "A set of states $S$: continuous or discrete\n",
    "A set of actions $A$: continuous or discrete\n",
    "A set of rewards $R$: continuous or discrete\n",
    "For each state $s \\in S$ there are permitted actions $a \\in A(s)$\n",
    "Transition Probabilities: $p(s_{t+1}, r_{t+1}|s_t, a_t)$\n",
    "\n",
    "With this version we can calculate the probability of moving to certain state $s_{t+1}$ and acquiring reward $r_{t+1}$, given that we are in the state $s_t$ and do action $a_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP definition ver 2\n",
    "A set of states $S$: continuous or discrete\n",
    "A set of actions $A$: continuous or discrete\n",
    "A set of rewards $R$: continuous or discrete\n",
    "For each state $s \\in S$ there are permitted actions $a \\in A(s)$\n",
    "Transition Probabilities: $p(s_{t+1}|s_t, a_t)$\n",
    "Deterministic function of $r_{t+1} = f(s_t, s_{t+1}, a_t)$\n",
    "\n",
    "With this version we can calculate the probability of moving to certain state $s_{t+1}$, given that we are in the state $s_t$ and do action $a_t$. And we can also calculate the reward $r_{t+1}$ from performing $a_t$ in $s_{t+1}$ and finishing in $s_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.1: MDP ver1 vs ver2\n",
    "The most notable difference between those two definitions is the fact that ver 1 calculates already possibility of ending up in $s_{t+1}, r_{t+1}$, while ver 2 calculates first probability of ending up in the $s_{t+1}$, and then we can get the deterministic function to calculate the expected reward $r_{t+1}$. Also the ver 2 does not work with a stochastic reward system, by that I mean that for ver 2 to work for every $s_{t+1}, s_t, a_t$ there is deterministic reward $r_{t+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.2: generic stochastic MDP\n",
    "This Python Class for generic MDP implements formal definition of MDP ver 2 (states, actions, rewards, permitted actions, transition probabilities, reward function). Variables below are added to illustrate the behaviour of MDP. Permitted actions are solved by using the input (`transition_probabilities`, `rewards`) that consist only of permitted actions and `lookup_transition_probability`, `lookup_reward` functions which in case of forbidden action return adequate value. In addition, for next assignments `start_state`, `terminal_states` and `random_termination` were implemented. The structure of variables will be shown in next assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T19:06:19.046646400Z",
     "start_time": "2023-12-01T19:06:18.624700900Z"
    }
   },
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    def __init__(self, states, actions, transition_probabilities, rewards, start_state, gamma=0.9,\n",
    "                 eps=1e6, random_termination=0.0, cost_of_living=0.0):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.transition_probabilities = transition_probabilities\n",
    "        self.inspect_probabilities()\n",
    "        self.rewards = rewards\n",
    "        self.start_state = start_state\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.random_termination = random_termination\n",
    "        assert 0 <= self.random_termination <= 1\n",
    "        self.cost_of_living = cost_of_living\n",
    "\n",
    "        self.value = {}\n",
    "        for state in states:\n",
    "            self.value[state] = 0.0\n",
    "\n",
    "    # def reset(self):\n",
    "    #     for state in self.states:\n",
    "    #         self.value[state] = 0.0\n",
    "\n",
    "    def lookup_transition_probability(self, state: str, action: str, next_state: str):\n",
    "        return self.transition_probabilities[state].get(action, {}).get(next_state, 0.0)\n",
    "\n",
    "    def lookup_reward(self, state: str, action: str, next_state: str):\n",
    "        return self.transition_probabilities[state].get(action, {}).get(next_state, 0)\n",
    "\n",
    "    def inspect_probabilities(self):\n",
    "        for state in self.transition_probabilities.values():\n",
    "            for action in state.values():\n",
    "                assert isclose(sum(action.values()), 1, abs_tol=1e-4)\n",
    "\n",
    "    # def value(self, state: str):\n",
    "    #     pass\n",
    "\n",
    "    # def action_value(self, state: str, action: str):\n",
    "    #     next_states = self.transition_probabilities[state].get(action, {})\n",
    "    #     return sum(self.lookup_transition_probability(state, action, next_state) * (\n",
    "    #             self.lookup_reward(state, action, next_state) + self.gamma * self.value[next_state]) for next_state\n",
    "    #                in next_states)\n",
    "    #\n",
    "    # def policy_random(self):\n",
    "    #     return random.choice(self.actions)\n",
    "    #\n",
    "    # def estimate_value(self):\n",
    "    #     for _ in range(int(self.eps)):\n",
    "    #         for state in self.states:\n",
    "    #             self.value[state] = self.action_value(state, self.policy_random())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.3: MDP form image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below uses states, actions, rewards, and probabilities form the image. It was a simple rewriting values form the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T19:06:19.046646400Z",
     "start_time": "2023-12-01T19:06:18.652662200Z"
    }
   },
   "outputs": [],
   "source": [
    "states = ['s0', 's1', 's2']\n",
    "actions = ['a0', 'a1']\n",
    "\n",
    "transition_probabilities = {\n",
    "    's0': {'a0': {'s0': 0.5, 's2': 0.5},\n",
    "           'a1': {'s2': 1}},\n",
    "    's1': {'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "           'a1': {'s1': 0.95, 's2': 0.05}},\n",
    "    's2': {'a0': {'s0': 0.4, 's2': 0.6},\n",
    "           'a1': {'s0': 0.3, 's1': 0.3, 's2': 0.4}},\n",
    "}\n",
    "\n",
    "rewards = {\n",
    "    's0': {},\n",
    "    's1': {'a0': {'s0': 5}},\n",
    "    's2': {\n",
    "        'a1': {'s0': -1}},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2\n",
    "Clarification Note:\n",
    "`is_slippery` is named random_termination, as I believe that it better reflects the purpose of this variable.\n",
    "Terminal States, are not directly present as for example special array. Terminal states are states that do not have any actions available in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2.1\n",
    "Squares are numbered from 0 to 4. Permitted actions are left (l) and right (r) if they are not transitioning the agent outside of the environment. `random_termination`, `cost_of_living` can be specified by user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T19:06:19.046646400Z",
     "start_time": "2023-12-01T19:06:18.691542700Z"
    }
   },
   "outputs": [],
   "source": [
    "states = ['0', '1', '2', '3', '4']\n",
    "actions = ['l', 'r']\n",
    "\n",
    "# Terminal States have no actions\n",
    "transition_probabilities = {\n",
    "    '0': {},\n",
    "    '1': {'l': {'0': 1},\n",
    "          'r': {'2': 1}},\n",
    "    '2': {'l': {'1': 1},\n",
    "          'r': {'3': 1}},\n",
    "    '3': {'l': {'2': 1},\n",
    "          'r': {'4': 1}},\n",
    "    '4': {},\n",
    "}\n",
    "\n",
    "rewards = {\n",
    "    '1': {'l': {'0': -1}},\n",
    "    '3': {'r': {'4': 1}},\n",
    "}\n",
    "\n",
    "mdp = MDP(states, actions, transition_probabilities, rewards, '2', random_termination=0.3, cost_of_living=-1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2.2\n",
    "States are represented by x and y coordinates on the grid. Actions indicate move on the x or y axis. `transition_probabilities` are generated, as writing them by hand would be waste of time. Rewards follow the same strategy. Bottom states are also implemented. Solution can be scaled for larger grids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T19:06:19.046646400Z",
     "start_time": "2023-12-01T19:06:18.725171700Z"
    }
   },
   "outputs": [],
   "source": [
    "# # print the states\n",
    "# for y in range(3):\n",
    "#     for x in range(5):\n",
    "#         print((x, y), end=\", \")\n",
    "#     print(\"\")\n",
    "\n",
    "# grid world (x, y)\n",
    "states = [(0, 0), (1, 0), (2, 0), (3, 0), (4, 0),\n",
    "          (0, 1), (1, 1), (2, 1), (3, 1), (4, 1),\n",
    "          (0, 2), (1, 2), (2, 2), (3, 2), (4, 2)]\n",
    "# right, left, up, down\n",
    "actions = [(1, 0), (-1, 0), (0, 1), (0, -1)]\n",
    "\n",
    "transition_probabilities = {}\n",
    "\n",
    "# generate transition_probabilities\n",
    "for state in states:\n",
    "    transition_probabilities[state] = {}\n",
    "    for action in actions:\n",
    "        if 0 <= state[0] + action[0] <= 4 and 0 <= state[1] + action[1] <= 2:\n",
    "            transition_probabilities[state][action] = {(state[0] + action[0], state[1] + action[1]): 1}\n",
    "\n",
    "# terminal states\n",
    "for state in [(1, 2), (2, 2), (3, 2), (4, 2)]:\n",
    "    transition_probabilities[state] = {}\n",
    "\n",
    "# same structure for rewards\n",
    "rewards = copy.deepcopy(transition_probabilities)\n",
    "\n",
    "for state in rewards:\n",
    "    amount_of_actions = len(rewards[state])\n",
    "    for action in rewards[state]:\n",
    "        for next_state in rewards[state][action]:\n",
    "            if next_state in [(1, 2), (2, 2), (3, 2)]:\n",
    "                rewards[state][action][next_state] = -1\n",
    "            if next_state == (4, 2):\n",
    "                rewards[state][action][next_state] = 1\n",
    "            else:\n",
    "                rewards[state][action][next_state] = 0\n",
    "\n",
    "# start\n",
    "start_state = (0, 2)\n",
    "\n",
    "mdp = MDP(states, actions, transition_probabilities, rewards, start_state, random_termination=0.3,\n",
    "          cost_of_living=-1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2.3\n",
    "States are represented by the coordinates x and y and Boolean for recognising if agent obtained the key. Key is obtained by going from (2, 1, False) to (2, 0, True), stating that user obtained the key. In addition, state (2, 0, False) was introduced, although it may seem pointless, this state can be theoretically obtained by agent starting in the (2, 0), and with the rule that \"Key is obtained by going to this (key state) state\".Transition form the state (3, 1) to (4, 1) is not possible without the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T19:06:19.046646400Z",
     "start_time": "2023-12-01T19:06:18.742204400Z"
    }
   },
   "outputs": [],
   "source": [
    "# # print the states\n",
    "# for y in range(3):\n",
    "#     for x in range(5):\n",
    "#         for z in [True, False]:\n",
    "#             print((x, y, z), end=\", \")\n",
    "#     print(\"\")\n",
    "\n",
    "# grid world (x, y)\n",
    "# if can start in the key state, but not obtaining the key at the same time\n",
    "# states = [(2, 0, True), (2, 0, False),\n",
    "states = [(2, 0, True),\n",
    "          (0, 1, True), (0, 1, False), (1, 1, True), (1, 1, False), (2, 1, True), (2, 1, False), (3, 1, True),\n",
    "          (3, 1, False), (4, 1, True)]\n",
    "# right, left, up, down\n",
    "actions = [(1, 0), (-1, 0), (0, 1), (0, -1)]\n",
    "\n",
    "transition_probabilities = {}\n",
    "\n",
    "# generate transition_probabilities\n",
    "for state in states:\n",
    "    transition_probabilities[state] = {}\n",
    "    for action in actions:\n",
    "        if action[1] == 0 and state[1] == 1 and (state[0] + action[0], state[1] + action[1], state[2]) in states:\n",
    "            transition_probabilities[state][action] = {(state[0] + action[0], state[1] + action[1], state[2]): 1}\n",
    "\n",
    "# this depends if we say that key is obtained by going to this state, or being in it\n",
    "# transition_probabilities[(2, 0, False)][(0, 1)] = {(2, 1, True): 1}\n",
    "\n",
    "# without the key to the key state\n",
    "transition_probabilities[(2, 1, False)][(0, -1)] = {(2, 0, True): 1}\n",
    "# with the key to the key state\n",
    "transition_probabilities[(2, 1, True)][(0, -1)] = {(2, 0, True): 1}\n",
    "\n",
    "# from the key state to state lower\n",
    "transition_probabilities[(2, 0, True)][(0, 1)] = {(2, 1, True): 1}\n",
    "\n",
    "# transition to the green state\n",
    "transition_probabilities[(3, 1, True)][(1, 0)] = {(4, 1, True): 1}\n",
    "\n",
    "# terminal states\n",
    "transition_probabilities[(0, 1, True)] = {}\n",
    "transition_probabilities[(0, 1, False)] = {}\n",
    "transition_probabilities[(4, 1, True)] = {}\n",
    "\n",
    "# Rewards\n",
    "rewards = {}\n",
    "\n",
    "for state in transition_probabilities:\n",
    "    rewards[state] = {}\n",
    "    for action in transition_probabilities[state]:\n",
    "        for next_state in transition_probabilities[state][action]:\n",
    "            if next_state in [(0, 1, True), (0, 1, False)]:\n",
    "                rewards[state][action] = {next_state: -1}\n",
    "            if next_state == (4, 1, True):\n",
    "                rewards[state][action] = {next_state: 1}\n",
    "\n",
    "# start\n",
    "start_state = (0, 2)\n",
    "\n",
    "mdp = MDP(states, actions, transition_probabilities, rewards, start_state, random_termination=0.3,\n",
    "          cost_of_living=-1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 3.1.1\n",
    "For agent with random move policy, he can walk for the eternity, by choosing between for example state $s_1$ and $s_2$. For this reason I have provided the general formulas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 3.1.2\n",
    "Agent from the states $s_1$, $s_2$, $s_3$ can go to the $s_4$, but if the agent is already in one of the terminal states ($s_0$, $s_4$), he cannot proceed further, but while calculating $v$, from what I understood I omit the terminal states, so this lead to answers below. $s_4$ has 0, as if agent goes to $s_4$, he cannot go from $s_4$ to $s_4$, unless we allow him to stay in place as an action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3.2\n",
    "Here are all equations. Explanation below.\n",
    "![equations](img/full.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 3.2.1\n",
    "Cost of living (`-0.1`) should be reduced after every step, rest is similar, like in Assignment 3.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 3.2.2\n",
    "Cost of living (`-2`) should be reduced after every step, rest is similar, like in Assignment 3.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 3.2.3\n",
    "For the cost of living `-2` it makes sense for agent to go to the orange state from the leftmost gray position, as it will allow him to get only penalty of `-3` instead of `-5` if he went for the green state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 3.3.1\n",
    "Similar, like in Assignment 3.1.1, but with $q$ function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 3.3.2\n",
    "Similar, like in Assignment 3.1.2, but with $q$ function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 3.4.1\n",
    "If the Agent changes the policy to taking the highest $q$ it will always go for the green state, except for the case with cost of living `-2`. &q$ and $v$ will reflect that by counting returns for agent that tries to get to the green state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 3.4.2\n",
    "Yes, I believe that results make sens for the given policies. Here I also consider the general formulas as the result, as counting infinite number of steps would be hard, so given that they make sens."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
