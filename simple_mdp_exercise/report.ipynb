{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Simple MDP Design\n",
    "Grzegorz Malisz\n",
    "Student Number: 4852370"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Assignment 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MDP definition ver 1\n",
    "A set of states $S$: continuous or discrete\n",
    "A set of actions $A$: continuous or discrete\n",
    "A set of rewards $R$: continuous or discrete\n",
    "For each state $s \\in S$ there are permitted actions $a \\in A(s)$\n",
    "Transition Probabilities: $p(s_{t+1}, r_{t+1}|s_t, a_t)$\n",
    "\n",
    "With this version we can calculate the probability of moving to certain state $s_{t+1}$ and acquiring reward $r_{t+1}$, given that we are in the state $s_t$ and do action $a_t$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MDP definition ver 2\n",
    "A set of states $S$: continuous or discrete\n",
    "A set of actions $A$: continuous or discrete\n",
    "A set of rewards $R$: continuous or discrete\n",
    "For each state $s \\in S$ there are permitted actions $a \\in A(s)$\n",
    "Transition Probabilities: $p(s_{t+1}|s_t, a_t)$\n",
    "Deterministic function of $r_{t+1} = f(s_t, s_{t+1}, a_t)$\n",
    "\n",
    "With this version we can calculate the probability of moving to certain state $s_{t+1}$, given that we are in the state $s_t$ and do action $a_t$. And we can also calculate the reward $r_{t+1}$ from performing $a_t$ in $s_{t+1}$ and finishing in $s_t$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Assignment 1.1: MDP ver1 vs ver2\n",
    "The most notable difference between those two definitions is the fact that ver 1 calculates already possibility of ending up in $s_{t+1}, r_{t+1}$, while ver 2 calculates first probability of ending up in the $s_{t+1}$, and then we can get the deterministic function to calculate the expected reward $r_{t+1}$. Also the ver 2 does not work with a stochastic reward system, by that I mean that for ver 2 to work for every $s_{t+1}, s_t, a_t$ there is deterministic reward $r_{t+1}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Assignment 1.2: generic stochastic MDP\n",
    "This Python Class for generic MDP implements formal definition of MDP ver 2. Variables below are added to illustrate the behaviour of MDP. Permitted actions are solved by using the input (`transition_probabilities`, `rewards`) that consist only of permitted actions and `lookup_transition_probability`, `lookup_reward` functions which in case of forbidden action return adequate value."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    def __init__(self, states, actions, transition_probabilities, rewards):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.transition_probabilities = transition_probabilities\n",
    "        self.rewards = rewards\n",
    "\n",
    "    def lookup_transition_probability(self, state: str, action: str, next_state: str):\n",
    "        return self.transition_probabilities[state].get(action, {}).get(next_state, 0.0)\n",
    "\n",
    "    def lookup_reward(self, state: str, action: str, next_state: str):\n",
    "        return self.transition_probabilities[state].get(action, {}).get(next_state, 0)\n",
    "\n",
    "states = ['s1', 's2', 's3']\n",
    "actions = ['a', 'b', 'c']\n",
    "\n",
    "transition_probabilities = {\n",
    "    's1': {'a': {'s1': 0.5, 's2': 0.5}},\n",
    "    's2': {'a': {'s1': 0.1, 's2': 0.4, 's3': 0.5},\n",
    "           'b': {'s1': 0.6, 's2': 0.3, 's3': 0.1}},\n",
    "    's3': {'b': {'s2': 1.0}}\n",
    "}\n",
    "\n",
    "rewards = {\n",
    "    's1': {'a': {'s1': 5, 's2': 10}},\n",
    "    's2': {'a': {'s1': -2, 's2': 1, 's3': 4},\n",
    "           'b': {'s1': 3, 's2': 2, 's3': 8}},\n",
    "    's3': {'b': {'s2': 100}}\n",
    "}\n",
    "\n",
    "mdp_generic = MDP(states, actions, transition_probabilities=transition_probabilities, rewards=rewards)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T16:33:19.178225800Z",
     "start_time": "2023-11-26T16:33:19.164618Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Assignment 1.3: MDP form image"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Code below uses states, actions, rewards, and probabilities form the image."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "states = ['s0', 's1', 's2']\n",
    "actions = ['a0', 'a1']\n",
    "\n",
    "transition_probabilities = {\n",
    "    's0': {'a0': {'s0': 0.5, 's2': 0.5},\n",
    "           'a1': {'s2': 1}},\n",
    "    's1': {'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "           'a1': {'s1': 0.95, 's2': 0.05}},\n",
    "    's2': {'a0': {'s0': 0.4, 's2': 0.6},\n",
    "           'a1': {'s0': 0.3, 's1': 0.3, 's2': 0.4}},\n",
    "}\n",
    "\n",
    "rewards = {\n",
    "    's0': {},\n",
    "    's1': {'a0': {'s0': 5}},\n",
    "    's2': {\n",
    "        'a1': {'s0': -1}},\n",
    "}\n",
    "\n",
    "mdp_from_image = MDP(states, actions, transition_probabilities=transition_probabilities, rewards=rewards)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Assignment 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Assignment 2.1\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Assignment 3\n",
    "### Assignment 3.1\n",
    "$$\n",
    "v(s_0) = \\mathbb{E}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
